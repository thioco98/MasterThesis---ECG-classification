{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impact of varying the number of leads in ECGs classification using deep learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import wfdb\n",
    "import numpy as np\n",
    "import ecg_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Data visualization using WFDB package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = wfdb.rdrecord(\"Training_Data/WFDB_ChapmanShaoxing/JS00001\")\n",
    "print(record.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Sizes and samples for all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_CSPC = \"D:/ULB/MA2/Master thesis/Training_Data/WFDB_CPSC2018\"\n",
    "directory_CSPC2 = \"D:/ULB/MA2/Master thesis/Training_Data/WFDB_CPSC2018_2\"\n",
    "directory_PTB = \"D:/ULB/MA2/Master thesis/Training_Data/WFDB_PTB\"\n",
    "directory_PTBXL = \"D:/ULB/MA2/Master thesis/Training_Data/WFDB_PTBXL\"\n",
    "directory_StPet = \"D:/ULB/MA2/Master thesis/Training_Data/WFDB_StPetersburg\"\n",
    "dir_list = [directory_CSPC,directory_CSPC2,directory_PTB,directory_PTBXL,directory_StPet]\n",
    "\n",
    "\n",
    "for i in range(len(dir_list)) :\n",
    "    data_X, data_Y = load_challenge_data(dir_list[i])\n",
    "    all_diag, data_Y_bin = binarizing(data_Y)\n",
    "    var_name = dir_list[i].split(\"/\")\n",
    "    print(var_name[len(var_name)-1]+\": \")\n",
    "    print(len(data_X),len(data_X[0]))\n",
    "    print(len(all_diag))\n",
    "    print(np.shape(np.array(min(data_X, key=lambda x: np.shape(np.array(x))[1])))[1])\n",
    "    print(np.shape(np.array(max(data_X, key=lambda x: np.shape(np.array(x))[1])))[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Distribution of the diagnoses in PTB and PTB-XL datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"D:/ULB/MA2/Master thesis/Training_Data/WFDB_PTBXL\"\n",
    "max_numb_samples = 20000\n",
    "train_X, train_Y, header_data = load_challenge_data(directory, max_numb_samples)\n",
    "train_Y_plot = []\n",
    "for lst in train_Y:\n",
    "    for element in lst :\n",
    "        train_Y_plot.append(element)\n",
    "train_Y_occ = []\n",
    "for i in range(len(all_diag)):\n",
    "    train_Y_occ.append(np.count_nonzero(np.array(train_Y_plot) == all_diag[i]))\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,5.5)\n",
    "plt.bar(all_diag,train_Y_occ)\n",
    "plt.xticks(list(range(len(all_diag))), all_diag, rotation='vertical')\n",
    "plt.xlabel(\"Diagnosis in dataset\")\n",
    "plt.ylabel(\"Number of occurences\")\n",
    "plt.show()\n",
    "\n",
    "all_samp = []\n",
    "for lst in train_X :\n",
    "    all_samp.append(str(np.shape(np.array(lst))[1]))\n",
    "all_samp_uni = np.unique(np.array(all_samp))\n",
    "train_Y_samp = []\n",
    "for element in all_samp_uni :\n",
    "    train_Y_samp.append(np.count_nonzero(np.array(all_samp) == element))\n",
    "plt.bar(list(all_samp_uni),train_Y_samp)\n",
    "plt.xticks(list(range(len(all_samp_uni))), list(all_samp_uni), rotation='vertical')\n",
    "plt.xlabel(\"Samples sizes\")\n",
    "plt.ylabel(\"Number of occurences\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 ECG plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_plot.plot(train_X[0][:,0:3000]/1000,sample_rate=500,title='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import wfdb\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_challenge_data(directory,max_numb_samples):\n",
    "    data_X = []\n",
    "    data_Y = []\n",
    "    header_data_lst = []\n",
    "    i = 0\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".mat\"):\n",
    "            i += 1\n",
    "            if i > max_numb_samples :\n",
    "                break\n",
    "            x = loadmat(directory+\"/\"+file)\n",
    "            data = np.asarray(x['val'], dtype=np.float64)\n",
    "            new_file = file.replace('.mat','.hea')\n",
    "            input_header_file = os.path.join(directory+\"/\"+new_file)\n",
    "            with open(input_header_file,'r') as f:\n",
    "                header_data=f.readlines()\n",
    "            for line in header_data :\n",
    "                if \"#Dx: \" in line :\n",
    "                    data_Y_part = line.split(\"Dx: \")[1].split(\"\\n\")[0].split(\",\")\n",
    "            header_data_lst.append(header_data)\n",
    "            data_X.append(data)\n",
    "            data_Y.append(data_Y_part)\n",
    "    return data_X, data_Y, header_data_lst\n",
    "\n",
    "def binarizing(dataset) :\n",
    "    all_diag = []\n",
    "    for i in range(len(dataset)):\n",
    "        all_diag = list(set(all_diag + dataset[i]))\n",
    "    bin_data = np.zeros((len(dataset),len(all_diag)))\n",
    "    for i in range(len(dataset)) :\n",
    "        for j in range(len(dataset[i])) :\n",
    "            bin_data[i,np.where(np.array(all_diag) == str(dataset[i][j]))[0][0]] = 1\n",
    "    return all_diag, np.asarray(bin_data, dtype = np.int8)\n",
    "\n",
    "def keeping_bin_class(dataset, all_diag, spec_value):\n",
    "    ind = np.where(np.array(all_diag) == spec_value)[0]\n",
    "    return dataset[:,ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_challenge_data_2(dir_list,max_numb_samples,k):\n",
    "    data_X = []\n",
    "    data_Y = []\n",
    "    if (k+1)*max_numb_samples >= len(dir_list) :\n",
    "        maxi = len(dir_list)\n",
    "    else :\n",
    "        maxi = (k+1)*max_numb_samples\n",
    "    for i in range(k*max_numb_samples,maxi):\n",
    "        file = dir_list[i]\n",
    "        x = loadmat(directory+\"/\"+file)\n",
    "        data = np.asarray(x['val'], dtype=np.float64)\n",
    "        new_file = file.replace('.mat','.hea')\n",
    "        input_header_file = os.path.join(directory+\"/\"+new_file)\n",
    "        with open(input_header_file,'r') as f:\n",
    "            header_data=f.readlines()\n",
    "        for line in header_data :\n",
    "            if \"#Dx: \" in line :\n",
    "                data_Y_part = line.split(\"Dx: \")[1].split(\"\\n\")[0].split(\",\")\n",
    "        data_X.append(data)\n",
    "        data_Y.append(data_Y_part)\n",
    "    return data_X, data_Y\n",
    "\n",
    "def binarizing(dataset) :\n",
    "    all_diag = []\n",
    "    for i in range(len(dataset)):\n",
    "        all_diag = list(set(all_diag + dataset[i]))\n",
    "    bin_data = np.zeros((len(dataset),len(all_diag)))\n",
    "    for i in range(len(dataset)) :\n",
    "        for j in range(len(dataset[i])) :\n",
    "            bin_data[i,np.where(np.array(all_diag) == str(dataset[i][j]))[0][0]] = 1\n",
    "    return all_diag, np.asarray(bin_data, dtype = np.int8)\n",
    "\n",
    "def keeping_bin_class(dataset, all_diag, spec_value):\n",
    "    ind = np.where(np.array(all_diag) == spec_value)[0]\n",
    "    return dataset[:,ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import wfdb\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "from math import ceil\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from scipy.signal import kaiserord, firwin, butter, iirnotch, filtfilt, freqz, resample\n",
    "from matplotlib import pyplot as plt\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_normalization(dataset):\n",
    "    max_value = np.amax(dataset)\n",
    "    min_value = np.amin(dataset)\n",
    "    return (dataset*2-max_value-min_value)/(max_value-min_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data equalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_equalizing(dataset_X,dataset_Y, eq_numb):\n",
    "    all_diag = np.unique(dataset_Y,axis=0)\n",
    "    new_dataset_Y = []\n",
    "    new_dataset_X = []\n",
    "    for i in range(len(all_diag)):\n",
    "        j = 0\n",
    "        for k in range(len(dataset_Y)):\n",
    "            if np.array_equal(dataset_Y[k],all_diag[i]):\n",
    "                new_dataset_Y.append(dataset_Y[k])\n",
    "                new_dataset_X.append(dataset_X[k])\n",
    "                j += 1\n",
    "            if j >= min(np.count_nonzero(dataset_Y==all_diag[i]),eq_numb):\n",
    "                break\n",
    "    return np.array(new_dataset_X), np.array(new_dataset_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Weights computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculating_class_weights(dataset):\n",
    "    number_dim = np.shape(dataset)[1]\n",
    "    weights = np.empty([number_dim, 2])\n",
    "    for i in range(number_dim):\n",
    "        weights[i] = compute_class_weight(class_weight='balanced', classes=[0,1], y=dataset[:, i])\n",
    "    keys = np.arange(0,len(weights),1)\n",
    "    weight_dictionary = dict(zip(keys, weights.T[1]))\n",
    "    return weight_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Data filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_filtering(dataset, sample_freq, cutoff_high, cutoff_low, powerline, type_bandpass):\n",
    "    nyq_freq = sample_freq/2\n",
    "    b, a = iirnotch(powerline, 20, fs=sample_freq)\n",
    "    dataset = filtfilt(b, a, dataset)\n",
    "    if type_bandpass == \"bw\" :\n",
    "        d, c = butter(3, [cutoff_low/nyq_freq, cutoff_high/nyq_freq], 'band')\n",
    "        dataset = filtfilt(d, c, dataset, method='gust')\n",
    "    elif type_bandpass == \"fir\" :\n",
    "        FIR_filter = firwin(40, [cutoff_low/nyq_freq, cutoff_high/nyq_freq], pass_zero=False)\n",
    "        dataset_filt = []\n",
    "        for i in range(np.shape(dataset)[0]):\n",
    "            dataset_filt_part = []\n",
    "            for j in range(np.shape(dataset)[1]):\n",
    "                dataset_filt_part.append(np.convolve(FIR_filter, dataset[i][j], mode='valid'))\n",
    "            dataset_filt.append(deepcopy(dataset_filt_part))\n",
    "        dataset = np.array(dataset_filt)\n",
    "    return dataset, b, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Diagnoses choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diseases_choice(diseases_list, all_diag, dataset_Y, dataset_X):\n",
    "    new_dataset_Y = []\n",
    "    new_dataset_X = []\n",
    "    new_new_dataset_Y = []\n",
    "    for element in diseases_list :\n",
    "        ind = all_diag.index(element)\n",
    "        new_dataset_Y.append(dataset_Y[:,ind])\n",
    "    new_dataset_Y = np.array(new_dataset_Y).T\n",
    "    for j in range(len(diseases_list)) :\n",
    "        for i in range(len(new_dataset_Y)) :\n",
    "            if new_dataset_Y[i][j] == 1 :\n",
    "                new_new_dataset_Y.append(new_dataset_Y[i])\n",
    "                new_dataset_X.append(dataset_X[i])\n",
    "    return np.array(new_dataset_X),np.array(new_new_dataset_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Data augmentation (by cropping and downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_augmentation(dataset_X, dataset_Y, inter_value):\n",
    "    dataset_X_new = []\n",
    "    dataset_Y_new = []\n",
    "    for i in range(len(dataset_X)):\n",
    "        init_value = 0\n",
    "        data_size = np.shape(np.array(dataset_X[i]))[1]\n",
    "        for j in range(ceil(data_size/inter_value)):\n",
    "            if data_size <= init_value + inter_value :\n",
    "                dataset_X_new.append(np.array(dataset_X[i])[:,data_size-inter_value:data_size])\n",
    "            else :\n",
    "                dataset_X_new.append(np.array(dataset_X[i])[:,init_value:init_value + inter_value])\n",
    "            dataset_Y_new.append(dataset_Y[i])\n",
    "            init_value += inter_value\n",
    "    return np.array(dataset_X_new), np.array(dataset_Y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsampling(data_X, data_Y):\n",
    "    new_data_X = []\n",
    "    new_data_Y = []\n",
    "    for i in range(len(data_X)):\n",
    "        new_data_X_part1 = []\n",
    "        new_data_X_part2 = []\n",
    "        for j in range(len(data_X[i])):\n",
    "            new_data_X_part1.append(np.array(resample(data_X[i][j], int(len(data_X[i][j])/2))))\n",
    "            new_data_X_part2.append(np.array(resample(data_X[i][j][1:len(data_X[i][j])], int(len(data_X[i][j])/2))))\n",
    "        new_data_X.append(np.array(deepcopy(new_data_X_part1)))\n",
    "        new_data_X.append(np.array(deepcopy(new_data_X_part2)))\n",
    "        new_data_Y.append(data_Y[i])\n",
    "        new_data_Y.append(data_Y[i])\n",
    "    return np.array(new_data_X), np.array(new_data_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Number of leads choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choosing_numb_leads(data_X,numb_leads):\n",
    "    new_data_X = []\n",
    "    if numb_leads == 8 :\n",
    "        for i in range(len(data_X)):\n",
    "            new_data_X.append(np.concatenate((data_X[i][0:2],data_X[i][6:12])))\n",
    "    elif numb_leads == 2 :\n",
    "        for i in range(len(data_X)):\n",
    "            new_data_X.append(data_X[i][0:2])\n",
    "    elif numb_leads == 12 :\n",
    "        new_data_X = data_X\n",
    "    else :\n",
    "        print(\"Not a good number of leads, possibilities: 2, 8 or 12.\")\n",
    "    return np.array(new_data_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Transposition of the ECGs matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transposition(dataset):\n",
    "    new_dataset = []\n",
    "    for i in range(len(dataset)):\n",
    "        new_dataset.append(dataset[i].T)\n",
    "    print(np.shape(new_dataset))\n",
    "    return np.array(new_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Removing data both healthy and sick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removing_11(data_X,data_Y):\n",
    "    remov = np.array([1,1])\n",
    "    ind_list = []\n",
    "    for i in range(len(data_Y)) :\n",
    "        if np.array_equal(data_Y[i],remov) :\n",
    "            ind_list.append(i)\n",
    "    for i in range(len(ind_list)-1,-1,-1):\n",
    "        data_Y = np.delete(data_Y,ind_list[i],0)\n",
    "        data_X = np.delete(data_X,ind_list[i],0)\n",
    "    return data_X, data_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 Dataset summarized preprocessing and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_bin(directory, max_numb_samples, eq_numb, diseases_list):\n",
    "    dir_list = os.listdir(directory)\n",
    "    dir_list = [element for element in dir_list if element.endswith(\".mat\")] \n",
    "    dir_list_len = len(dir_list)\n",
    "    all_diag_fin = []\n",
    "    for i in range(ceil(dir_list_len/max_numb_samples)):\n",
    "        print(\"Batch number {0} out of {1} in preparation\".format(i+1,ceil(dir_list_len/max_numb_samples)))\n",
    "        dataset_X, dataset_Y = load_challenge_data_2(dir_list, max_numb_samples, i)\n",
    "        all_diag, dataset_Y = binarizing(dataset_Y)\n",
    "        all_diag_fin = list(set(all_diag_fin + deepcopy(all_diag)))\n",
    "        dataset_X, b, a = data_filtering(dataset_X, 500, 60, 10, 50, \"bw\")\n",
    "        dataset_X = data_normalization(dataset_X)\n",
    "        if len(diseases_list) != 0:\n",
    "            dataset_X, dataset_Y = diseases_choice(diseases_list, all_diag, dataset_Y, dataset_X)\n",
    "        dataset_X, dataset_Y = data_equalizing(dataset_X,dataset_Y,eq_numb)\n",
    "        weights = calculating_class_weights(dataset_Y)\n",
    "        dataset_X_reshaped = dataset_X.reshape(dataset_X.shape[0], -1)\n",
    "        np.savetxt(\"DATA PROCESSED/X/test_X_\"+str(i)+\".txt\", dataset_X_reshaped)\n",
    "        np.savetxt(\"DATA PROCESSED/Y/test_Y_\"+str(i)+\".txt\", dataset_Y)\n",
    "    print(\"Number of diagnoses : \"+str(len(np.unique(all_diag_fin))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_multi(directory, max_numb_samples, eq_numb, diseases_list,all_diag):\n",
    "    dir_list = os.listdir(directory)\n",
    "    dir_list = [element for element in dir_list if element.endswith(\".mat\")] \n",
    "    dir_list_len = len(dir_list)\n",
    "    for i in range(ceil(dir_list_len/max_numb_samples)):\n",
    "        print(\"Batch number {0} out of {1} in preparation\".format(i+1,ceil(dir_list_len/max_numb_samples)))\n",
    "        dataset_X, dataset_Y = load_challenge_data_2(dir_list, max_numb_samples, i)\n",
    "        all_diag, dataset_Y = binarizing(dataset_Y,all_diag)\n",
    "        dataset_X, b, a = data_filtering(dataset_X, 500, 60, 10, 50, \"bw\")\n",
    "        dataset_X = deepcopy(data_normalization(dataset_X))\n",
    "        if len(diseases_list) != 0:\n",
    "            dataset_X, dataset_Y = diseases_choice(diseases_list, all_diag, dataset_Y, dataset_X)\n",
    "        dataset_X, dataset_Y = data_equalizing(dataset_X,dataset_Y,eq_numb)\n",
    "        weights = calculating_class_weights(dataset_Y)\n",
    "        dataset_X_reshaped = dataset_X.reshape(dataset_X.shape[0], -1)\n",
    "        np.savetxt(\"DATA PROCESSED MULTI/X/test_X_\"+str(i)+\".txt\", dataset_X_reshaped)\n",
    "        np.savetxt(\"DATA PROCESSED MULTI/Y/test_Y_\"+str(i)+\".txt\", dataset_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 Loading preprocessed data and splitting in sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load_and_split_bin():\n",
    "    shape = 5000\n",
    "    data_X = []\n",
    "    data_Y = []\n",
    "    for file in os.listdir(\"DATA PROCESSED/X\"):\n",
    "        data_part = np.loadtxt(\"DATA PROCESSED/X/\"+file)\n",
    "        data_part = data_part.reshape(data_part.shape[0], data_part.shape[1] // shape, shape)\n",
    "        data_X.append(data_part)\n",
    "    data_X = np.vstack(data_X)\n",
    "    for file in os.listdir(\"DATA PROCESSED/Y\"):\n",
    "        data_part = np.loadtxt(\"DATA PROCESSED/Y/\"+file)\n",
    "        data_Y.append(data_part)\n",
    "    data_Y = np.vstack(data_Y)\n",
    "    return data_X, data_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load_and_split_multi():\n",
    "    shape = 5000\n",
    "    data_X = []\n",
    "    data_Y = []\n",
    "    for file in os.listdir(\"DATA PROCESSED MULTI/X\"):\n",
    "        print(file)\n",
    "        data_part = np.loadtxt(\"DATA PROCESSED MULTI/X/\"+file)\n",
    "        data_part = data_part.reshape(data_part.shape[0], data_part.shape[1] // shape, shape)\n",
    "        data_part = crop_augmentation(data_part,500)\n",
    "        data_part = downsampling(data_part)\n",
    "        data_X.append(data_part)\n",
    "    data_X = np.vstack(data_X)\n",
    "    for file in os.listdir(\"DATA PROCESSED MULTI/Y\"):\n",
    "        print(file)\n",
    "        data_part = np.loadtxt(\"DATA PROCESSED MULTI/Y/\"+file)\n",
    "        data_Y.append(data_part)\n",
    "    data_Y = np.vstack(data_Y)\n",
    "    return data_X, data_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Models implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, BatchNormalization, Dense, Dropout, GlobalAveragePooling1D, MaxPool1D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Convolutional neural network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_bin(input_size, output_size,lr):\n",
    "    inputlayer = keras.layers.Input(shape=input_size) \n",
    "    conv1 = keras.layers.Conv1D(filters=128, kernel_size=8,input_shape=(input_size), padding='same')(inputlayer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.Activation(activation='relu')(conv1)\n",
    "    conv2 = keras.layers.Conv1D(filters=256, kernel_size=5, padding='same')(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.Activation('relu')(conv2)\n",
    "    conv3 = keras.layers.Conv1D(128, kernel_size=3,padding='same')(conv2)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.Activation('relu')(conv3)\n",
    "    gap_layer = keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "    outputlayer = keras.layers.Dense(output_size, activation='sigmoid')(gap_layer)\n",
    "    model = keras.Model(inputs=inputlayer, outputs=outputlayer)\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=lr), metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), tf.keras.metrics.AUC(num_thresholds=200,curve=\"ROC\",summation_method=\"interpolation\",name=\"AUC\",dtype=None,thresholds=None,multi_label=True,label_weights=None,)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_multi(input_size, output_size,lr):\n",
    "    inputlayer = keras.layers.Input(shape=input_size) \n",
    "    conv1 = keras.layers.Conv1D(filters=128, kernel_size=8,input_shape=(input_size), padding='same')(inputlayer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.Activation(activation='relu')(conv1)\n",
    "    conv2 = keras.layers.Conv1D(filters=256, kernel_size=5, padding='same')(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.Activation('relu')(conv2)\n",
    "    conv3 = keras.layers.Conv1D(128, kernel_size=3,padding='same')(conv2)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.Activation('relu')(conv3)\n",
    "    gap_layer = keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "    outputlayer = keras.layers.Dense(output_size, activation='softmax')(gap_layer)\n",
    "    model = keras.Model(inputs=inputlayer, outputs=outputlayer)\n",
    "    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=lr), metrics=[tf.keras.metrics.CategoricalAccuracy(name='accuracy', dtype=None),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), tf.keras.metrics.AUC(num_thresholds=200,curve=\"ROC\",summation_method=\"interpolation\",name=\"AUC\",dtype=None,thresholds=None,multi_label=True,label_weights=None,)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Residual neural network (ResNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_net_block(input_layer,nb_filters):\n",
    "    x = keras.layers.Conv1D(filters=nb_filters, kernel_size=8, padding='same')(input_layer)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    x = keras.layers.Conv1D(filters=nb_filters, kernel_size=5, padding='same')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    x = keras.layers.Conv1D(filters=nb_filters, kernel_size=3, padding='same')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_network_1d_bin(input_size,output_size,nb_filters):\n",
    "    input_shape = input_size\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    # BLOCK 1\n",
    "    x = res_net_block(input_layer,nb_filters)\n",
    "    shortcut = keras.layers.Conv1D(filters=nb_filters, kernel_size=1, padding='same')(input_layer)\n",
    "    shortcut = keras.layers.BatchNormalization()(shortcut)\n",
    "    output_block_1 = keras.layers.Concatenate()([shortcut, x])\n",
    "    output_block_1 = keras.layers.Activation('relu')(output_block_1)\n",
    "\n",
    "    # BLOCK 2\n",
    "    x = res_net_block(output_block_1,nb_filters*2)\n",
    "    shortcut = keras.layers.Conv1D(filters=nb_filters*2, kernel_size=1, padding='same')(output_block_1)\n",
    "    shortcut = keras.layers.BatchNormalization()(shortcut)\n",
    "    output_block_2 = keras.layers.Concatenate()([shortcut, x])\n",
    "    output_block_2 = keras.layers.Activation('relu')(output_block_2)\n",
    "\n",
    "    # BLOCK 3\n",
    "    x = res_net_block(output_block_2,nb_filters*2)\n",
    "    # shortcut = keras.layers.Conv1D(filters=nb_filters*2, kernel_size=1, padding='same')(output_block_2)\n",
    "    shortcut = keras.layers.BatchNormalization()(output_block_2)\n",
    "    output_block_3 = keras.layers.Concatenate()([shortcut, x])\n",
    "    output_block_3 = keras.layers.Activation('relu')(output_block_3)\n",
    "\n",
    "    # OUTPUT BLOCK\n",
    "    gap_layer = keras.layers.GlobalAveragePooling1D()(output_block_3)\n",
    "    output_layer = keras.layers.Dense(output_size, activation='sigmoid')(gap_layer)\n",
    "    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.000001), metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), tf.keras.metrics.AUC(num_thresholds=200,curve=\"ROC\",summation_method=\"interpolation\",name=\"AUC\",dtype=None,thresholds=None,multi_label=True,label_weights=None,)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_network_1d_multi(input_size,output_size,nb_filters):\n",
    "    input_shape = input_size\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    # BLOCK 1\n",
    "    x = res_net_block(input_layer,nb_filters)\n",
    "    shortcut = keras.layers.Conv1D(filters=nb_filters, kernel_size=1, padding='same')(input_layer)\n",
    "    shortcut = keras.layers.BatchNormalization()(shortcut)\n",
    "    output_block_1 = keras.layers.Concatenate()([shortcut, x])\n",
    "    output_block_1 = keras.layers.Activation('relu')(output_block_1)\n",
    "\n",
    "    # BLOCK 2\n",
    "    x = res_net_block(output_block_1,nb_filters*2)\n",
    "    shortcut = keras.layers.Conv1D(filters=nb_filters*2, kernel_size=1, padding='same')(output_block_1)\n",
    "    shortcut = keras.layers.BatchNormalization()(shortcut)\n",
    "    output_block_2 = keras.layers.Concatenate()([shortcut, x])\n",
    "    output_block_2 = keras.layers.Activation('relu')(output_block_2)\n",
    "\n",
    "    # BLOCK 3\n",
    "    x = res_net_block(output_block_2,nb_filters*2)\n",
    "    # shortcut = keras.layers.Conv1D(filters=nb_filters*2, kernel_size=1, padding='same')(output_block_2)\n",
    "    shortcut = keras.layers.BatchNormalization()(output_block_2)\n",
    "    output_block_3 = keras.layers.Concatenate()([shortcut, x])\n",
    "    output_block_3 = keras.layers.Activation('relu')(output_block_3)\n",
    "\n",
    "    # OUTPUT BLOCK\n",
    "    gap_layer = keras.layers.GlobalAveragePooling1D()(output_block_3)\n",
    "    output_layer = keras.layers.Dense(output_size, activation='softmax')(gap_layer)\n",
    "    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.000001), metrics=[tf.keras.metrics.CategoricalAccuracy(name='accuracy', dtype=None),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), tf.keras.metrics.AUC(num_thresholds=200,curve=\"ROC\",summation_method=\"interpolation\",name=\"AUC\",dtype=None,thresholds=None,multi_label=True,label_weights=None,)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Transformer encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_model_bin(input_size,output_size,lr):\n",
    "    input_layer = keras.layers.Input(shape=input_size)\n",
    "    if input_size[1] == 2 :\n",
    "        pool = 1\n",
    "    else :\n",
    "        pool = 2\n",
    "\n",
    "     # conv block -1\n",
    "    conv1 = keras.layers.Conv1D(filters=128,kernel_size=5,strides=1,padding='same')(input_layer)\n",
    "    conv1 = tfa.layers.InstanceNormalization()(conv1)\n",
    "    conv1 = keras.layers.PReLU(shared_axes=[1])(conv1)\n",
    "    conv1 = keras.layers.Dropout(rate=0.2)(conv1)\n",
    "    conv1 = keras.layers.MaxPooling1D(pool_size=pool)(conv1)\n",
    "    # conv block -2\n",
    "    conv2 = keras.layers.Conv1D(filters=256,kernel_size=11,strides=1,padding='same')(conv1)\n",
    "    conv2 = tfa.layers.InstanceNormalization()(conv2)\n",
    "    conv2 = keras.layers.PReLU(shared_axes=[1])(conv2)\n",
    "    conv2 = keras.layers.Dropout(rate=0.2)(conv2)\n",
    "    conv2 = keras.layers.MaxPooling1D(pool_size=pool)(conv2)\n",
    "    # conv block -3\n",
    "    conv3 = keras.layers.Conv1D(filters=512,kernel_size=21,strides=1,padding='same')(conv2)\n",
    "    conv3 = tfa.layers.InstanceNormalization()(conv3)\n",
    "    conv3 = keras.layers.PReLU(shared_axes=[1])(conv3)\n",
    "    conv3 = keras.layers.Dropout(rate=0.5)(conv3)\n",
    "    # split for attention\n",
    "    attention_data = keras.layers.Lambda(lambda x: x[:,:,:256])(conv3)\n",
    "    attention_softmax = keras.layers.Lambda(lambda x: x[:,:,256:])(conv3)\n",
    "    # attention mechanism\n",
    "    attention_softmax = keras.layers.Softmax()(attention_softmax)\n",
    "    multiply_layer = keras.layers.Multiply()([attention_softmax,attention_data])\n",
    "    # last layer\n",
    "    dense_layer = keras.layers.Dense(units=256,activation='sigmoid')(multiply_layer)\n",
    "    dense_layer = tfa.layers.InstanceNormalization()(dense_layer)\n",
    "    # output layer\n",
    "    flatten_layer = keras.layers.Flatten()(dense_layer)\n",
    "    output_layer = keras.layers.Dense(units=output_size,activation='sigmoid')(flatten_layer)\n",
    "    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=lr), metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), tf.keras.metrics.AUC(num_thresholds=200,curve=\"ROC\",summation_method=\"interpolation\",name=\"AUC\",dtype=None,thresholds=None,multi_label=True,label_weights=None,)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_model_multi(input_size,output_size,lr):\n",
    "    input_layer = keras.layers.Input(shape=input_size)\n",
    "    if input_size[1] == 2 :\n",
    "        pool = 1\n",
    "    else :\n",
    "        pool = 2\n",
    "\n",
    "     # conv block -1\n",
    "    conv1 = keras.layers.Conv1D(filters=128,kernel_size=5,strides=1,padding='same')(input_layer)\n",
    "    conv1 = tfa.layers.InstanceNormalization()(conv1)\n",
    "    conv1 = keras.layers.PReLU(shared_axes=[1])(conv1)\n",
    "    conv1 = keras.layers.Dropout(rate=0.2)(conv1)\n",
    "    conv1 = keras.layers.MaxPooling1D(pool_size=pool)(conv1)\n",
    "    # conv block -2\n",
    "    conv2 = keras.layers.Conv1D(filters=256,kernel_size=11,strides=1,padding='same')(conv1)\n",
    "    conv2 = tfa.layers.InstanceNormalization()(conv2)\n",
    "    conv2 = keras.layers.PReLU(shared_axes=[1])(conv2)\n",
    "    conv2 = keras.layers.Dropout(rate=0.2)(conv2)\n",
    "    conv2 = keras.layers.MaxPooling1D(pool_size=pool)(conv2)\n",
    "    # conv block -3\n",
    "    conv3 = keras.layers.Conv1D(filters=512,kernel_size=21,strides=1,padding='same')(conv2)\n",
    "    conv3 = tfa.layers.InstanceNormalization()(conv3)\n",
    "    conv3 = keras.layers.PReLU(shared_axes=[1])(conv3)\n",
    "    conv3 = keras.layers.Dropout(rate=0.5)(conv3)\n",
    "    # split for attention\n",
    "    attention_data = keras.layers.Lambda(lambda x: x[:,:,:256])(conv3)\n",
    "    attention_softmax = keras.layers.Lambda(lambda x: x[:,:,256:])(conv3)\n",
    "    # attention mechanism\n",
    "    attention_softmax = keras.layers.Softmax()(attention_softmax)\n",
    "    multiply_layer = keras.layers.Multiply()([attention_softmax,attention_data])\n",
    "    # last layer\n",
    "    dense_layer = keras.layers.Dense(units=256,activation='softmax')(multiply_layer)\n",
    "    dense_layer = tfa.layers.InstanceNormalization()(dense_layer)\n",
    "    # output layer\n",
    "    flatten_layer = keras.layers.Flatten()(dense_layer)\n",
    "    output_layer = keras.layers.Dense(units=output_size,activation='softmax')(flatten_layer)\n",
    "    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=lr), metrics=[tf.keras.metrics.CategoricalAccuracy(name='accuracy', dtype=None),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), tf.keras.metrics.AUC(num_thresholds=200,curve=\"ROC\",summation_method=\"interpolation\",name=\"AUC\",dtype=None,thresholds=None,multi_label=True,label_weights=None,)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Bidirectional gated recurrent unit (BiGRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_bin(input_size,output_size,lr):\n",
    "    input_layer = keras.layers.Input(shape=input_size)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=256, recurrent_dropout=0.1, dropout=0.1, return_sequences=True))(input_layer)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=256, recurrent_dropout=0.1, dropout=0.1, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=256, recurrent_dropout=0.1, dropout=0.1, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=128, recurrent_dropout=0.1, dropout=0.1, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=128, recurrent_dropout=0.1, dropout=0.1, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=128, recurrent_dropout=0.1, dropout=0.1, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(256,activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    output_layer = tf.keras.layers.Dense(output_size,activation='sigmoid')(x)\n",
    "    model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=lr), metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy', dtype=None, threshold=0.5),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), tf.keras.metrics.AUC(num_thresholds=200,curve=\"ROC\",summation_method=\"interpolation\",name=\"AUC\",dtype=None,thresholds=None,multi_label=True,label_weights=None,)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_multi(input_size,output_size,lr):\n",
    "    input_layer = keras.layers.Input(shape=input_size)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=256, recurrent_dropout=0.1, dropout=0.1, return_sequences=True))(input_layer)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=256, recurrent_dropout=0.1, dropout=0.1, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=256, recurrent_dropout=0.1, dropout=0.1, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=128, recurrent_dropout=0.1, dropout=0.1, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=128, recurrent_dropout=0.1, dropout=0.1, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=128, recurrent_dropout=0.1, dropout=0.1, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(256,activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    output_layer = tf.keras.layers.Dense(output_size,activation='softmax')(x)\n",
    "    model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=lr), metrics=[tf.keras.metrics.CategoricalAccuracy(name='accuracy', dtype=None),tf.keras.metrics.Recall(name='Recall'),tf.keras.metrics.Precision(name='Precision'), tf.keras.metrics.AUC(num_thresholds=200,curve=\"ROC\",summation_method=\"interpolation\",name=\"AUC\",dtype=None,thresholds=None,multi_label=True,label_weights=None,)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results and graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_plots(hist_list,name):\n",
    "    label_list = ['Training accuracy - 2 leads','Validation accuracy - 2 leads','Training accuracy - 8 leads','Validation accuracy - 8 leads','Training accuracy - 12 leads','Validation accuracy - 12 leads']\n",
    "    color_list = ['b','r','g','c','m','k']\n",
    "    plt.figure(figsize=[8,6])\n",
    "    for i in range(len(hist_list)):\n",
    "        history = hist_list[i]\n",
    "        plt.plot(history.history['accuracy'],color_list[i*2],linewidth=3.0,label=label_list[i*2])\n",
    "        plt.plot(history.history['val_accuracy'],color_list[i*2+1],linewidth=3.0,label=label_list[i*2+1])\n",
    "    plt.xlabel('Epochs ',fontsize=16)\n",
    "    plt.ylabel('Accuracy',fontsize=16)\n",
    "    plt.title('Accuracy Curves - '+name,fontsize=16)\n",
    "    plt.ylim([0.5,1])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_plots(hist_list,name):\n",
    "    label_list = ['Training loss - 2 leads','Validation Loss - 2 leads','Training loss - 8 leads','Validation Loss - 8 leads','Training loss - 12 leads','Validation Loss - 12 leads']\n",
    "    color_list = ['b','r','g','c','m','k']\n",
    "    plt.figure(figsize=[8,6])\n",
    "    for i in range(len(hist_list)):\n",
    "        history = hist_list[i]\n",
    "        plt.plot(history.history['loss'],color_list[i*2],linewidth=3.0,label=label_list[i*2])\n",
    "        plt.plot(history.history['val_loss'],color_list[i*2+1],linewidth=3.0,label=label_list[i*2+1])\n",
    "    plt.xlabel('Epochs ',fontsize=16)\n",
    "    plt.ylabel('Loss',fontsize=16)\n",
    "    plt.title('Loss Curves - '+name,fontsize=16)\n",
    "    plt.ylim([0,3])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plots_for_all_leads_multi():\n",
    "    nb_leads_list = [2,8,12]\n",
    "    for i in range(3,4):\n",
    "        hist_list = []\n",
    "        for nb_leads in nb_leads_list :\n",
    "            data_X_leads = choosing_numb_leads(data_X,nb_leads)\n",
    "            if len(np.shape(data_Y)) > 1 :\n",
    "                output_size = np.shape(data_Y)[1]\n",
    "            else :\n",
    "                output_size = 1\n",
    "            input_size = (np.shape(data_X_leads)[1],np.shape(data_X_leads)[2])\n",
    "            \n",
    "            if i == 0 :\n",
    "                data_X_leads = transposition(data_X_leads)\n",
    "                input_size = (np.shape(data_X_leads)[1],np.shape(data_X_leads)[2])\n",
    "                model = cnn_multi(input_size,output_size,0.000002)\n",
    "                nb_epochs = 200\n",
    "                name = \"CNN\"\n",
    "                x_train,x_rem,y_train,y_rem = train_test_split(data_X_leads,data_Y,train_size=.80, shuffle=True, random_state=42)\n",
    "                x_val, x_test, y_val, y_test = train_test_split(x_rem,y_rem,train_size=.80, shuffle=True, random_state=42)\n",
    "            elif i == 1 :\n",
    "                data_X_leads = transposition(data_X_leads)\n",
    "                input_size = (np.shape(data_X_leads)[1],np.shape(data_X_leads)[2])\n",
    "                model = encoder_model_multi(input_size,output_size,0.00005)\n",
    "                nb_epochs = 50\n",
    "                name = \"Transformer encoder\"\n",
    "                x_train,x_rem,y_train,y_rem = train_test_split(data_X_leads,data_Y,train_size=.80, shuffle=True, random_state=42)\n",
    "                x_val, x_test, y_val, y_test = train_test_split(x_rem,y_rem,train_size=.80, shuffle=True, random_state=42)\n",
    "            elif i == 2 :\n",
    "                data_X_leads = choosing_numb_leads(data_X,nb_leads)\n",
    "                input_size = (np.shape(data_X_leads)[1],np.shape(data_X_leads)[2])\n",
    "                model = residual_network_1d_multi(input_size,output_size,64)\n",
    "                nb_epochs = 200\n",
    "                name = \"ResNet\"\n",
    "                x_train,x_rem,y_train,y_rem = train_test_split(data_X_leads,data_Y,train_size=.80, shuffle=True, random_state=42)\n",
    "                x_val, x_test, y_val, y_test = train_test_split(x_rem,y_rem,train_size=.80, shuffle=True, random_state=42)\n",
    "            elif i == 3 :\n",
    "                data_X_leads = choosing_numb_leads(data_X,nb_leads)\n",
    "                input_size = (np.shape(data_X_leads)[1],np.shape(data_X_leads)[2])\n",
    "                model = rnn_multi(input_size,output_size,0.00001)\n",
    "                nb_epochs = 50\n",
    "                name = \"BiGRU\"\n",
    "                x_train,x_rem,y_train,y_rem = train_test_split(data_X_leads,data_Y,train_size=.80, shuffle=True, random_state=42)\n",
    "                x_val, x_test, y_val, y_test = train_test_split(x_rem,y_rem,train_size=.80, shuffle=True, random_state=42)\n",
    "            history = model.fit(x_train, y_train, epochs=nb_epochs, validation_data=(x_val,y_val), validation_freq=1)\n",
    "            hist_list.append(history)\n",
    "            predictions = model.predict(x_test)\n",
    "            print(\"Accuracy on test set for {0} model with {1} leads: {2} %\".format(name,nb_leads,accuracy_score(y_test,np.around(predictions))))\n",
    "        accuracy_plots(hist_list,name)\n",
    "        loss_plots(hist_list,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plots_for_all_leads():\n",
    "    nb_leads_list = [2,8,12]\n",
    "    for i in range(2,4):\n",
    "        hist_list = []\n",
    "        for nb_leads in nb_leads_list :\n",
    "            data_X_leads = choosing_numb_leads(data_X,nb_leads)\n",
    "            if len(np.shape(data_Y)) > 1 :\n",
    "                output_size = np.shape(data_Y)[1]\n",
    "            else :\n",
    "                output_size = 1\n",
    "            input_size = (np.shape(data_X_leads)[1],np.shape(data_X_leads)[2])\n",
    "            \n",
    "            if i == 0 :\n",
    "                data_X_leads = transposition(data_X_leads)\n",
    "                input_size = (np.shape(data_X_leads)[1],np.shape(data_X_leads)[2])\n",
    "                model = cnn_bin(input_size,output_size,0.000002)\n",
    "                nb_epochs = 10\n",
    "                name = \"CNN\"\n",
    "                x_train,x_rem,y_train,y_rem = train_test_split(data_X_leads,data_Y,train_size=.80, shuffle=True, random_state=100,stratify=data_Y)\n",
    "                x_val, x_test, y_val, y_test = train_test_split(x_rem,y_rem,train_size=.80, shuffle=True, random_state=100,stratify=y_rem)\n",
    "            elif i == 1 :\n",
    "                data_X_leads = transposition(data_X_leads)\n",
    "                input_size = (np.shape(data_X_leads)[1],np.shape(data_X_leads)[2])\n",
    "                model = encoder_model_bin(input_size,output_size,0.00005)\n",
    "                nb_epochs = 50\n",
    "                name = \"Transformer encoder\"\n",
    "                x_train,x_rem,y_train,y_rem = train_test_split(data_X_leads,data_Y,train_size=.80, shuffle=True, random_state=100,stratify=data_Y)\n",
    "                x_val, x_test, y_val, y_test = train_test_split(x_rem,y_rem,train_size=.80, shuffle=True, random_state=100,stratify=y_rem)\n",
    "            elif i == 2 :\n",
    "                data_X_leads = choosing_numb_leads(data_X,nb_leads)\n",
    "                input_size = (np.shape(data_X_leads)[1],np.shape(data_X_leads)[2])\n",
    "                model = residual_network_1d_bin(input_size,output_size,64)\n",
    "                nb_epochs = 50\n",
    "                name = \"ResNet\"\n",
    "                x_train,x_rem,y_train,y_rem = train_test_split(data_X_leads,data_Y,train_size=.80, shuffle=True, random_state=100,stratify=data_Y)\n",
    "                x_val, x_test, y_val, y_test = train_test_split(x_rem,y_rem,train_size=.80, shuffle=True, random_state=100,stratify=y_rem)\n",
    "            elif i == 3 :\n",
    "                data_X_leads = choosing_numb_leads(data_X,nb_leads)\n",
    "                input_size = (np.shape(data_X_leads)[1],np.shape(data_X_leads)[2])\n",
    "                model = rnn_bin(input_size,output_size,0.00001)\n",
    "                nb_epochs = 50\n",
    "                name = \"BiGRU\"\n",
    "                x_train,x_rem,y_train,y_rem = train_test_split(data_X_leads,data_Y,train_size=.80, shuffle=True, random_state=100,stratify=data_Y)\n",
    "                x_val, x_test, y_val, y_test = train_test_split(x_rem,y_rem,train_size=.80, shuffle=True, random_state=100,stratify=y_rem)\n",
    "            # x_train,x_val,y_train,y_val = train_test_split(x_train_val,y_train_val,train_size=.66, shuffle=True, random_state=100)\n",
    "            history = model.fit(x_train, y_train, epochs=nb_epochs, validation_data=(x_val,y_val), validation_freq=1)\n",
    "            hist_list.append(history)\n",
    "            predictions = model.predict(x_test)\n",
    "            matrix = confusion_matrix(y_test, np.around(predictions))\n",
    "            print(\"Accuracy on test set for {0} model with {1} leads: {2} %\".format(name,nb_leads,(matrix[0][0]+matrix[1][1])*100/(matrix[0][0]+matrix[1][1]+matrix[0][1]+matrix[1][0])))\n",
    "        accuracy_plots(hist_list,name)\n",
    "        loss_plots(hist_list,name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Filtering effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Notch filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_filt, b, a = data_filtering(train_X, 500, 60, 10, 50, \"fir\")\n",
    "freq, h = freqz(b, a, fs=2*np.pi)\n",
    "samp_freq = 500\n",
    "plt.plot(freq*samp_freq/(2*np.pi), 20 * np.log10(abs(h)),'r', label='Bandpass filter', linewidth='2')\n",
    "plt.xlabel('Frequency [Hz]', fontsize=20)\n",
    "plt.ylabel('Magnitude [dB]', fontsize=20)\n",
    "plt.title('Notch Filter', fontsize=20)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 FIR and Butterworth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_filt_bw, b, a = data_filtering(data_X[0], 500, 100, 3, 50, \"bw\")\n",
    "train_X_filt_bw2, b, a = data_filtering(data_X[0], 500, 60, 10, 50, \"bw\")\n",
    "plt.plot(np.arange(len(data_X[0][0][0:500]))/500,data_X[0][0][0:500],'b',label=\"Original data\")\n",
    "plt.plot(np.arange(len(data_X[0][0][0:500]))/500,train_X_filt_bw[0][0:500],'g',label=\"Filtered data in the range [3,100] Hz\")\n",
    "plt.plot(np.arange(len(data_X[0][0][0:500]))/500,train_X_filt_bw2[0][0:500],'r',label=\"Filtered data in the range [10,60] Hz\")\n",
    "plt.xlabel('Time (s.)')\n",
    "plt.ylabel('Signal amplitude normalized')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(train_X_norm[0][0])\n",
    "k = np.arange(n)\n",
    "T = n/500\n",
    "frq = k/T\n",
    "frq = frq[:len(frq)//2]\n",
    "\n",
    "Y_1 = np.fft.fft(train_X_norm[0][0])/n\n",
    "Y_1 = Y_1[:n//2]\n",
    "Y_2 = np.fft.fft(train_X_filt_bw[0])/n \n",
    "Y_2 = Y_2[:n//2]\n",
    "Y_3 = np.fft.fft(train_X_filt_fir[0][0])/n \n",
    "Y_3 = Y_3[:n//2]\n",
    "plt.plot(frq,abs(Y_1),'b',label=\"Original signal\",alpha=0.7) \n",
    "plt.plot(frq,abs(Y_2),'g',label=\"Filtered by butterworth data\",alpha=1) \n",
    "plt.plot(frq,abs(Y_3),'r',label=\"Filtered by FIR data\",alpha=0.6) \n",
    "plt.xlabel('Frequence (Hz)')\n",
    "plt.ylabel('|Signal(frequence)|')\n",
    "plt.title(\"Fourier transforms of the signal and filtered signals\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9445cb2034dc1caab94b836cff50452aa696cf7e48c8fcd75b71a15675b3328"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
